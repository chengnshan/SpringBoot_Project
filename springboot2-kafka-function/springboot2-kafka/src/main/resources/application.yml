server:
  port: 8201
  servlet:
    context-path:

spring:
  kafka:
    # 以逗号分隔的地址列表，用于建立与Kafka集群的初始连接(kafka 默认的端口号为9092)
    bootstrap-servers: 192.168.153.136:9092
    #################producer的配置参数（开始）#################
    producer:
      # 如果该值大于零时，表示启用重试失败的发送次数,发生错误后，消息重发的次数。
      retries: 0
      #当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
      batch-size: 16384
      # 设置生产者内存缓冲区的大小。
      buffer-memory: 33554432
      # 键的序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 值的序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
      # acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
      # acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
      acks: 1
      #生产者生成的所有数据的压缩类型，none不压缩,此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
      compression-type: gzip
    #################producer的配置参数（结束）#################
    #################consumer的配置参数（开始）#################
    consumer:
      #ID在发出请求时传递给服务器;用于服务器端日志记录
     #client-id:
      # 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
      enable-auto-commit: true
       # 自动提交的时间间隔,默认值为5000,值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D
      auto-commit-interval: 1S
      # 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
      # latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
      # earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
      # none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。
      auto-offset-reset: earliest
      # 键的反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 值的反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      #如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）
      #默认值为500
      fetch-max-wait: 500ms
      #服务器应以字节为单位返回获取请求的最小数据量，默认值为1，对应的kafka的参数为fetch.min.bytes
      fetch-min-size: 1
      #心跳与消费者协调员之间的预期时间（以毫秒为单位），默认值为3000
      heartbeat-interval: 3000ms
      #一次调用poll()操作时返回的最大记录数，默认值为500
      maxPollRecords: 500
    #################consumer的配置参数（结束）#################
    #################listener的配置参数（结束）#################
    listener:
      # 在侦听器容器中运行的线程数。
      concurrency: 2
      #侦听器的AckMode,参见 https://docs.spring.io/spring-kafka/reference/htmlsingle/#committing-offsets
      #当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
      #RECORD   每处理一条commit一次
      #BATCH(默认)  每次poll的时候批量提交一次，频率取决于每次poll的调用频率
      #TIME   每次间隔ackTime的时间去commit(跟auto commit interval有什么区别呢？)
      #COUNT  累积达到ackCount次的ack去commit
      #COUNT_TIME   ackTime或ackCount哪个条件先满足，就commit
      #MANUAL   listener负责ack，但是背后也是批量上去
      #MANUAL_IMMEDIATE listner负责ack，每调用一次，就立即commit
    # ack-mode: manual
      type: single
      #轮询消费者时使用的超时（以毫秒为单位）
      poll-timeout: 5s
      #当ackMode为“COUNT”或“COUNT_TIME”时，偏移提交之间的记录数
    # ack-count:
      #当ackMode为“TIME”或“COUNT_TIME”时，偏移提交之间的时间（以毫秒为单位）
    # ack-time:
      #################listener的配置参数（结束）#################



